{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "172ab7e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import gutenberg\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "import string\n",
    "from collections import Counter, defaultdict\n",
    "import math\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6b1b6e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = ''.join([char for char in text if char not in string.punctuation])\n",
    "    return text\n",
    "\n",
    "def tokenize_text(text):\n",
    "    return word_tokenize(preprocess_text(text))\n",
    "\n",
    "def get_vocabulary(corpus):\n",
    "    vocabulary = set()\n",
    "    for doc in corpus:\n",
    "        words = tokenize_text(doc)\n",
    "        vocabulary.update(words)\n",
    "    return list(vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "664a9677",
   "metadata": {},
   "source": [
    "### Task 0:\n",
    "Take an arbitrary text from NLTK corpora (e.g. text3) and implement a Bag-of-Words tagger for it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b3d66e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bow_vectors(corpus):\n",
    "    vocabulary = get_vocabulary(corpus)\n",
    "    vocab_to_idx = {word: i for i, word in enumerate(vocabulary)}\n",
    "    \n",
    "    bow_vectors = []\n",
    "    for doc in corpus:\n",
    "        vector = np.zeros(len(vocabulary))\n",
    "        words = tokenize_text(doc)\n",
    "        word_counts = Counter(words)\n",
    "        \n",
    "        for word, count in word_counts.items():\n",
    "            if word in vocab_to_idx:\n",
    "                vector[vocab_to_idx[word]] = count\n",
    "        \n",
    "        bow_vectors.append(vector)\n",
    "    \n",
    "    return bow_vectors, vocabulary, vocab_to_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e78a99",
   "metadata": {},
   "source": [
    "### Task 1: \n",
    "Enhance the tagger so that it will use N-grams instead of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "41087da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ngram_bow_vectors(corpus, n=2):\n",
    "    all_ngrams = set()\n",
    "    \n",
    "    for doc in corpus:\n",
    "        words = tokenize_text(doc)\n",
    "        doc_ngrams = [' '.join(words[i:i+n]) for i in range(len(words)-n+1)]\n",
    "        all_ngrams.update(doc_ngrams)\n",
    "    \n",
    "    ngram_to_idx = {ngram: i for i, ngram in enumerate(all_ngrams)}\n",
    "    \n",
    "    ngram_bow_vectors = []\n",
    "    for doc in corpus:\n",
    "        vector = np.zeros(len(all_ngrams))\n",
    "        words = tokenize_text(doc)\n",
    "        doc_ngrams = [' '.join(words[i:i+n]) for i in range(len(words)-n+1)]\n",
    "        ngram_counts = Counter(doc_ngrams)\n",
    "        \n",
    "        for ngram, count in ngram_counts.items():\n",
    "            if ngram in ngram_to_idx:\n",
    "                vector[ngram_to_idx[ngram]] = count\n",
    "        \n",
    "        ngram_bow_vectors.append(vector)\n",
    "    \n",
    "    return ngram_bow_vectors, list(all_ngrams), ngram_to_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f89cde7e",
   "metadata": {},
   "source": [
    "### Task 2:\n",
    "Implement PPMI weighting with co-occurrence based on the presence within the same paragraph.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "450f26dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraphs = text.lower().split('\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "af8b209d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import math\n",
    "\n",
    "cooccur = defaultdict(lambda: defaultdict(int))\n",
    "word_freq = Counter()\n",
    "\n",
    "for para in paragraphs:\n",
    "    words = set(word_tokenize(para))\n",
    "    for w in words:\n",
    "        word_freq[w] += 1\n",
    "        for c in words:\n",
    "            if w != c:\n",
    "                cooccur[w][c] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ae3cdf2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_paragraphs = len(paragraphs)\n",
    "ppmi = defaultdict(dict)\n",
    "\n",
    "for w in cooccur:\n",
    "    for c in cooccur[w]:\n",
    "        p_wc = cooccur[w][c] / total_paragraphs\n",
    "        p_w = word_freq[w] / total_paragraphs\n",
    "        p_c = word_freq[c] / total_paragraphs\n",
    "        pmi = math.log2(p_wc / (p_w * p_c))\n",
    "        ppmi[w][c] = max(0, pmi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8dc6247",
   "metadata": {},
   "source": [
    "### Task 3:\n",
    "Implement PPMI weighting with co-occurrence based on a sliding window of neighboring words. Pick some number between 2-10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "98848556",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'filtered_words' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprobability\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FreqDist\n\u001b[0;32m      2\u001b[0m window_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m4\u001b[39m \n\u001b[1;32m----> 4\u001b[0m word_count \u001b[38;5;241m=\u001b[39m FreqDist(\u001b[43mfiltered_words\u001b[49m)\n\u001b[0;32m      5\u001b[0m cooccurrence \u001b[38;5;241m=\u001b[39m defaultdict(\u001b[38;5;28mint\u001b[39m)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(filtered_words)):\n",
      "\u001b[1;31mNameError\u001b[0m: name 'filtered_words' is not defined"
     ]
    }
   ],
   "source": [
    "from nltk.probability import FreqDist\n",
    "window_size = 4 \n",
    "\n",
    "word_count = FreqDist(filtered_words)\n",
    "cooccurrence = defaultdict(int)\n",
    "\n",
    "for i in range(len(filtered_words)):\n",
    "    center_word = filtered_words[i]\n",
    "    window_start = max(0, i - window_size)\n",
    "    window_end = min(len(filtered_words), i + window_size + 1)\n",
    "    for j in range(window_start, window_end):\n",
    "        if i == j:\n",
    "            continue\n",
    "        context_word = filtered_words[j]\n",
    "        pair = tuple(sorted((center_word, context_word)))\n",
    "        cooccurrence[pair] += 1\n",
    "\n",
    "\n",
    "total_pairs = sum(cooccurrence.values())\n",
    "\n",
    "ppmi = {}\n",
    "for pair, count in cooccurrence.items():\n",
    "    p_w1_w2 = count / total_pairs\n",
    "    p_w1 = word_count[pair[0]] / len(filtered_words)\n",
    "    p_w2 = word_count[pair[1]] / len(filtered_words)\n",
    "    pmi = math.log2(p_w1_w2 / (p_w1 * p_w2)) if p_w1_w2 > 0 else 0\n",
    "    ppmi[pair] = max(pmi, 0)\n",
    "\n",
    "print(f\"Top 10 PPMI pairs (window size = {window_size}):\")\n",
    "for pair, score in sorted(ppmi.items(), key=lambda x: x[1], reverse=True)[:10]:\n",
    "    print(f\"{pair}: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec86c638",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
